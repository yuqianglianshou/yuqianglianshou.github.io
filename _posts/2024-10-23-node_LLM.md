---
layout: post
title: LLM 笔记
date: 2024-10-23
tags: 笔记
---

### 富在术数，不在劳身，利在局势。

---

# NLP 基础与文本表示

- **NLP（自然语言处理 Natural Language Processinng）** 是让计算机理解与生成人类语言的学科，核心任务包括：
  - 文本分类、情感分析、命名实体识别（NER）
  - 文本生成、机器翻译、问答系统等
- **文本表示方式演化**：
  1. **One-hot / 词袋模型（BoW）**：简单但丢失语义。
  2. **Word2Vec / GloVe**：引入语义关系的词嵌入。
  3. **上下文表示（ELMo, BERT, GPT）**：动态词向量，语境敏感。

---

# Transformer 架构

Transformer 是 LLM 的基础，核心思想是 **注意力机制（Attention）**。
基于注意力机制的神经网络架构，
它通过“并行处理 + 注意力权重”高效地捕捉序列中词与词的关系，
成为 GPT、BERT 等大模型的基础。

- **Self-Attention**：每个词对输入序列中所有词计算相关性；
- **Multi-Head Attention**：并行多个注意力头，捕捉不同语义；
- **Position Encoding**：弥补序列位置信息的缺失；
- **结构类型**：
  - Encoder：理解任务（BERT）
  - Decoder：生成任务（GPT）
  - Encoder-Decoder：生成与理解并重（T5）

---

# 预训练语言模型（PLM）

- **Encoder-only**：如 BERT，用于句子理解类任务；
- **Decoder-only**：如 GPT 系列，用于文本生成；
- **Encoder-Decoder**：如 T5，可同时处理理解与生成。

预训练目标：

- **MLM（Masked Language Modeling）**
- **CLM（Causal Language Modeling）**
- **Seq2Seq 生成任务**

---

# 大语言模型（LLM）

**LLM 是具备亿级以上参数的深度生成模型。**

- **关键特征**：
  - 大规模参数量；
  - 长上下文理解能力；
  - 多任务泛化与“涌现能力”；
- **训练流程**：
  1. **预训练（Pretraining）**：学习语言规律；
  2. **指令微调（Instruction Tuning）**：适应特定任务；
  3. **RLHF（人类反馈强化学习）**：对齐人类偏好。

---

# 模型训练与微调

常用训练策略：

| 微调方法                      | 特点                 | 应用场景         |
| ----------------------------- | -------------------- | ---------------- |
| Full Fine-tune                | 全参数更新，计算量大 | 小模型、专用任务 |
| LoRA                          | 仅训练低秩矩阵       | 节省显存，效果佳 |
| QLoRA                         | 量化 + LoRA          | 适合消费级 GPU   |
| SFT（Supervised Fine-Tuning） | 有监督指令微调       | Align 模型行为   |
| RLHF                          | 人类反馈强化学习     | 对齐人类意图     |

---

# 大模型应用：RAG 与 Agent

- **RAG（Retrieval-Augmented Generation）**
  - 将知识检索与语言生成相结合；
  - 实现实时问答、知识库对话。
- **Agent**
  - 能主动执行任务的智能体；
  - 具备工具调用、推理、记忆与多步规划能力。

---

# 基本概念

- **BERT** 全名为 Bidirectional Encoder Representations from Transformers，是由 Google 团队在 2018年发布的预训练语言模型。该模型发布于论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，实现了包括 GLUE、MultiNLI 等七个自然语言处理评测任务的最优性能（State Of The Art，SOTA），堪称里程碑式的成果。自 BERT 推出以来，预训练+微调的模式开始成为自然语言处理任务的主流，不仅 BERT 自身在不断更新迭代提升模型性能，也出现了如 MacBERT、BART 等基于 BERT 进行优化提升的模型。可以说，BERT 是自然语言处理的一个阶段性成果，标志着各种自然语言处理任务的重大进展以及预训练模型的统治地位建立，一直到 LLM 的诞生，NLP 领域的主导地位才从 BERT 系模型进行迁移。即使在 LLM 时代，要深入理解 LLM 与 NLP，BERT 也是无法绕过的一环。

- **C4** 大规模数据集"Colossal Clean Crawled Corpus"(C4)，该数据集从Common Crawl中提取了大量干净的英语文本。C4数据集经过了一定的清洗，去除了无意义的文本、重复文本等。

- **CLM** 最传统也最直接的预训练任务——因果语言模型，Casual Language Model。CLM 可以看作 N-gram 语言模型的一个直接扩展。N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token，通过不断重复该过程来实现目标文本序列的生成。也就是说，CLM 是一个经典的补全形式。

- **tokenizer（分词器）** 分词器的作用是把自然语言输入切分成 token 并转化成一个固定的 index。

- **子词切分（Subword Segmentation）** 是 NLP 领域中的一种常见的文本预处理技术，旨在将词汇进一步分解为更小的单位，即子词。子词切分特别适用于处理词汇稀疏问题，即当遇到罕见词或未见过的新词时，能够通过已知的子词单位来理解或生成这些词汇。子词切分在处理那些拼写复杂、合成词多的语言（如德语）或者在预训练语言模型（如BERT、GPT系列）中尤为重要。
  子词切分的方法有很多种，常见的有Byte Pair Encoding (BPE)、WordPiece、Unigram、SentencePiece等。这些方法的基本思想是将单词分解成更小的、频繁出现的片段，这些片段可以是单个字符、字符组合或者词根和词缀。

- **BPE** 即 Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。例如，对“Hello World”这句话，可能会切分为“Hel，lo，Wor，ld”四个子词对。

- **词性标注（Part-of-Speech Tagging，POS Tagging）** 是 NLP 领域中的一项基础任务，它的目标是为文本中的每个单词分配一个词性标签，如名词、动词、形容词等。这个过程通常基于预先定义的词性标签集，如英语中的常见标签有名词（Noun，N）、动词（Verb，V）、形容词（Adjective，Adj）等。词性标注对于理解句子结构、进行句法分析、语义角色标注等高级NLP任务至关重要。通过词性标注，计算机可以更好地理解文本的含义，进而进行信息提取、情感分析、机器翻译等更复杂的处理。

- **文本分类（Text Classification）** 是 NLP 领域的一项核心任务，涉及到将给定的文本自动分配到一个或多个预定义的类别中。这项技术广泛应用于各种场景，包括但不限于情感分析、垃圾邮件检测、新闻分类、主题识别等。文本分类的关键在于理解文本的含义和上下文，并基于此将文本映射到特定的类别。

- **实体识别（Named Entity Recognition, NER）** 也称为命名实体识别，是 NLP 领域的一个关键任务，旨在自动识别文本中具有特定意义的实体，并将它们分类为预定义的类别，如人名、地点、组织、日期、时间等。实体识别任务对于信息提取、知识图谱构建、问答系统、内容推荐等应用很重要，它能够帮助系统理解文本中的关键元素及其属性。

- **关系抽取（Relation Extraction）** 是 NLP 领域中的一项关键任务，它的目标是从文本中识别实体之间的语义关系。这些关系可以是因果关系、拥有关系、亲属关系、地理位置关系等，关系抽取对于理解文本内容、构建知识图谱、提升机器理解语言的能力等方面具有重要意义。

- **文本摘要（Text Summarization）** 是 NLP 中的一个重要任务，目的是生成一段简洁准确的摘要，来概括原文的主要内容。根据生成方式的不同，文本摘要可以分为两大类：抽取式摘要（Extractive Summarization）和生成式摘要（Abstractive Summarization）。

- **机器翻译（Machine Translation, MT）** 是 NLP 领域的一项核心任务，指使用计算机程序将一种自然语言（源语言）自动翻译成另一种自然语言（目标语言）的过程。机器翻译不仅涉及到词汇的直接转换，更重要的是要准确传达源语言文本的语义、风格和文化背景等，使得翻译结果在目标语言中自然、准确、流畅，以便跨越语言障碍，促进不同语言使用者之间的交流与理解。

- **自动问答（Automatic Question Answering, QA）** 是 NLP 领域中的一个高级任务，旨在使计算机能够理解自然语言提出的问题，并根据给定的数据源自动提供准确的答案。自动问答任务模拟了人类理解和回答问题的能力，涵盖了从简单的事实查询到复杂的推理和解释。自动问答系统的构建涉及多个NLP子任务，如信息检索、文本理解、知识表示和推理等。
  自动问答大致可分为三类：检索式问答（Retrieval-based QA）、知识库问答（Knowledge-based QA）和社区问答（Community-based QA）。检索式问答通过搜索引擎等方式从大量文本中检索答案；知识库问答通过结构化的知识库来回答问题；社区问答则依赖于用户生成的问答数据，如问答社区、论坛等。
  自动问答系统的开发和优化是一个持续的过程，随着技术的进步和算法的改进，这些系统在准确性、理解能力和应用范围上都有显著的提升。通过结合不同类型的数据源和技术方法，自动问答系统正变得越来越智能，越来越能够处理复杂和多样化的问题。

- **向量空间模型（Vector Space Model, VSM）** 是 NLP 领域中一个基础且强大的文本表示方法，最早由哈佛大学Salton提出。向量空间模型通过将文本（包括单词、句子、段落或整个文档）转换为高维空间中的向量来实现文本的数学化表示。

- **N-gram 模型** 是 NLP 领域中一种基于统计的语言模型，广泛应用于语音识别、手写识别、拼写纠错、机器翻译和搜索引擎等众多任务。N-gram模型的核心思想是基于马尔可夫假设，即一个词的出现概率仅依赖于它前面的N-1个词。这里的N代表连续出现单词的数量，可以是任意正整数。例如，当N=1时，模型称为unigram，仅考虑单个词的概率；当N=2时，称为bigram，考虑前一个词来估计当前词的概率；当N=3时，称为trigram，考虑前两个词来估计第三个词的概率，以此类推N-gram。N-gram的优点是实现简单、容易理解，在许多任务中效果不错。但当N较大时，会出现数据稀疏性问题。模型的参数空间会急剧增大，相同的N-gram序列出现的概率变得非常低，导致模型无法有效学习，模型泛化能力下降。此外，N-gram模型忽略了词之间的范围依赖关系，无法捕捉到句子中的复杂结构和语义信息。

- **层归一化** 也就是 Layer Norm，是深度学习中经典的归一化操作。神经网络主流的归一化一般有两种，批归一化（Batch Norm）和层归一化（Layer Norm）。
  归一化核心是为了让不同层输入的取值范围或者分布能够比较一致。由于深度神经网络中每一层的输入都是上一层的输出，因此多层传递下，对网络中较高的层，之前的所有神经层的参数变化会导致其输入的分布发生较大的改变。也就是说，随着神经网络参数的更新，各层的输出分布是不相同的，且差异会随着网络深度的增大而增大。但是，需要预测的条件分布始终是相同的，从而也就造成了预测的误差。因此，在深度神经网络中，往往需要归一化操作，将每一层的输入都归一化成标准正态分布。
  **批归一化**是指在一个 mini-batch 上进行归一化，相当于对一个 batch 对样本拆分出来一部分。
  **层归一化（Layer Norm）**相较于 Batch Norm 在每一层统计所有样本的均值和方差，Layer Norm 在每个样本上计算其所有层的均值和方差，从而使每个样本的分布达到稳定。Layer Norm 的归一化方式其实和 Batch Norm 是完全一样的，只是统计统计量的维度不同。

- **残差连接** 即下一层的输入不仅是上一层的输出，还包括上一层的输入。残差连接允许最底层信息直接传到最高层，让高层专注于残差的学习。

- **涌现（emergence）** 通俗解释：当一个系统的规模、复杂度达到一定程度时，出现了原本在更小规模下根本没有的「新能力」或「新规律」。就像水分子本身没有“湿”的属性，但当许多水分子聚集在一起时，“湿”这个性质就出现了。这就是宏观层面涌现出微观层面没有的性质。
  在 LLM 中，“涌现”指的是：随着模型参数规模（比如 10亿 → 100亿 → 千亿）或训练数据量的增加，模型突然获得某种新能力，而不是能力逐步线性增长的结果。
  比如：

| 模型规模   | 能力表现                                     |
| ---------- | -------------------------------------------- |
| 1亿参数    | 会补全单词，简单对话                         |
| 10亿参数   | 会写简单句子，有点逻辑                       |
| 1000亿参数 | 突然能进行推理、翻译、写诗、编程、理解幽默…… |

这类“从不会到突然会”的非线性变化，就叫「涌现能力（emergent abilities）」。

- **batch,epoch**

| 名称                     | 含义                           | 举例               |
| ------------------------ | ------------------------------ | ------------------ |
| **sample（样本）**       | 一条数据，比如一句话、一篇文章 | “我爱学习AI”       |
| **batch（批）**          | 一组样本，用于一次参数更新     | 32句话             |
| **batch size（批大小）** | 每个批里样本的数量             | 32                 |
| **epoch（轮次）**        | 所有样本都训练一遍             | 数据集全部走完一次 |

<br/> 
<br/> 
<br/> 
<br/> 
<br/>
