<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>劉清揚</title>
    <description>欢迎来到我的个人站~</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 19 Aug 2024 12:02:29 +0800</pubDate>
    <lastBuildDate>Mon, 19 Aug 2024 12:02:29 +0800</lastBuildDate>
    <generator>Jekyll v4.3.3</generator>
    
    <item>
      <title>GPT-SoVITS 执行笔记</title>
      <description>&lt;h3 id=&quot;我追不上以前那个闪闪发光的自己了&quot;&gt;我追不上以前那个，闪闪发光的自己了。&lt;/h3&gt;

&lt;h2 id=&quot;效果展示&quot;&gt;效果展示&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1hS411w7rG/?spm_id_from=333.999.0.0&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【AI 语音-雪糕公主】 遗憾&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.bilibili.com/video/BV19M4m1y7Sb/?spm_id_from=333.788&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【AI 语音-罗翔老师】 遗憾&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;视频演示&quot;&gt;视频演示&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1NjYheuEqJ/?spm_id_from=333.999.0.0&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【GPTSoVits 语音克隆项目】（一）推理演示、效果对比&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;代码的下载与配置&quot;&gt;代码的下载与配置&lt;/h2&gt;

&lt;p&gt;GPT-SoVITS 项目地址： &lt;a href=&quot;https://github.com/RVC-Boss/GPT-SoVITS&quot;&gt;https://github.com/RVC-Boss/GPT-SoVITS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1，下载代码 ： https://github.com/RVC-Boss/GPT-SoVITS&lt;/p&gt;

&lt;p&gt;2，创建conda环境并启用：&lt;/p&gt;

&lt;p&gt;conda create -n GPTSoVits python=3.9
conda activate GPTSoVits&lt;/p&gt;

&lt;p&gt;3，安装依赖：  pip install -r requirements.txt&lt;/p&gt;

&lt;p&gt;4，下载需要文件并放好位置：&lt;/p&gt;

&lt;p&gt;ffmpeg.exe 放置在 GPT-SoVITS 根目录下：https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe
ffprobe.exe 放置在 GPT-SoVITS 根目录下：https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe&lt;/p&gt;

&lt;p&gt;下载预训练模型，并将它们放置在 GPT_SoVITS\pretrained_models 中：https://huggingface.co/lj1995/GPT-SoVITS/tree/main&lt;/p&gt;

&lt;p&gt;5,pytorch 下载：https://pytorch.org/get-started/locally/&lt;/p&gt;

&lt;p&gt;pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 –index-url https://download.pytorch.org/whl/cu118
注：这里指定了torch = 2.3.0 版本，默认安装的版本是2.4.0，训练模型时会报错：SystemError: initialization of _internal failed without raising an exception&lt;/p&gt;

&lt;h2 id=&quot;关于-问题建议&quot;&gt;关于 问题、建议&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;推理长语句自行做好分段，否则会有丢字或丢句的问题。&lt;/li&gt;
  &lt;li&gt;如果训练集干声不那么纯粹、干净，合成出来的语音会伴有电音，训练集越差，电音越明显，高音尤为明显。不那么差的情况下，可以多合成几次，选出来少好一些的，然后进行降噪处理。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Fri, 09 Aug 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/08/GPT-so-vits-%E6%89%A7%E8%A1%8C%E7%AC%94%E8%AE%B0/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/08/GPT-so-vits-%E6%89%A7%E8%A1%8C%E7%AC%94%E8%AE%B0/</guid>
        
      <category>技术_AI语音</category>
        
        
    </item>
    
    <item>
      <title>windows 激活</title>
      <description>&lt;h3 id=&quot;吾尝终日而思矣不如须臾之所学&quot;&gt;吾尝终日而思矣，不如须臾之所学。&lt;/h3&gt;

&lt;p&gt;视频教程：&lt;a href=&quot;https://www.bilibili.com/video/BV1Ae411W7Sz/?t=222.72536&amp;amp;spm_id_from=333.1350.jump_directly&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【果核剥壳】4句代码激活Windows和Office&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;windows-11-家庭中文版-激活演示&quot;&gt;Windows 11 家庭中文版 激活演示&lt;/h2&gt;

&lt;p&gt;通过管理员权限打开cmd，之后执行下面四句指令：&lt;/p&gt;

&lt;p&gt;1.卸载
slmgr.vbs /upk&lt;/p&gt;

&lt;p&gt;会提示成功卸载密钥，没有密钥的会提示找不到产品密钥。&lt;/p&gt;

&lt;p&gt;2.安装密钥
slmgr /ipk 密钥&lt;/p&gt;

&lt;p&gt;密钥请在下面密钥列表中找自己对应版本的密钥，例如：slmgr /ipk WMDGN-G9PQG-XVVXX-R3X43-63DFG，提示安装成功后进行下一步&lt;/p&gt;

&lt;p&gt;这里的秘钥需要自己找，&lt;a href=&quot;https://www.ghxi.com/kms.html/comment-page-8#comments&quot;&gt;https://www.ghxi.com/kms.html/comment-page-8#comments&lt;/a&gt; 这里能找到windows 11 版本以下的秘钥，我的window 11 秘钥没有，需要google 去搜，搜索最近发布的，找到未失效的即可，可能需要尝试很多个，这里我找到了一个，目前（2024.08.04）没有失效: PVMJN-6DFY6-9CCP6-7BKTT-D3WVR&lt;/p&gt;

&lt;p&gt;slmgr /ipk PVMJN-6DFY6-9CCP6-7BKTT-D3WVR&lt;/p&gt;

&lt;p&gt;3.设置激活服务器
slmgr /skms kms.ghxi.com&lt;/p&gt;

&lt;p&gt;提示设置成功后进行下一步&lt;/p&gt;

&lt;p&gt;4.开始激活
slmgr /ato&lt;/p&gt;

&lt;p&gt;输入后回车，稍等一会，会提示激活成功。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/08/windows-%E6%BF%80%E6%B4%BB/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/08/windows-%E6%BF%80%E6%B4%BB/</guid>
        
      <category>技术</category>
        
        
    </item>
    
    <item>
      <title>stable_diffusion_webui 提示词相关笔记</title>
      <description>&lt;h3 id=&quot;风能否向月而行&quot;&gt;风能否向月而行。&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;正向提示词：

(masterpiece:1,2),best quality,masterpiece,highres,original,extremely detailed wallpaper,perfect lighting,(extremely detremely detailed CG:1.2),drawing,paintbrush,

反向提示词：

 NSFW,(worst quality:2),(low quality:2),(normal quality:2),lowres,normal quality,((monochrome)),((grayscale)),skin spots,acnes,skin blemishes,age spot,(ugly:1.331),(duplicate:1.331),(morbid:1.21),(mutilated:1.21),(tranny:1.331),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.21),(bad proportions:1.331),extra limbs,(disfigured:1.331),(missing arms:1.331),(extra legs:1.331),(fused fingers:1.61051),(too many dingers:1.61051),(unclear eyes:1.331),lowers,bad hands,missing fingers,extra digit,bad hands,missing fingers,(((extra arms and legs)))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Fri, 10 May 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/05/stable_diffusion_webui-%E6%8F%90%E7%A4%BA%E8%AF%8D%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/05/stable_diffusion_webui-%E6%8F%90%E7%A4%BA%E8%AF%8D%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</guid>
        
      <category>技术_AI绘画</category>
        
        
    </item>
    
    <item>
      <title>stable_diffusion_webui 模型相关笔记</title>
      <description>&lt;h3 id=&quot;人这一生真的最怕原先晦涩难懂的歌词忽然有一天变得通俗&quot;&gt;人这一生，真的最怕原先晦涩难懂的歌词，忽然有一天变得通俗。。&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
模型总结，朋友们可对应在C站搜索下载（https://civitai.com/）

二次元：
1.最受欢迎的二次元模型：Anything
2精致度满满，室内外场景优秀：counterfeit
3.魔幻感十足：dreamlike diffusion

真实：
1.真实朴素：Realistic vision
2.照片级：Lofi
3.精细的写实风格：deliberate

2.5D
1.动漫角色的二次创作，即真实又二次元：never ending  dream
2.超现实的画面：Protogen x3.4 (Photorealism)
3.国风、小人书、水墨风：guofeng3

拓展：
富有现代感的建筑（dvArch - Multi-Prompt Archittecture Tuned Model)
富有魔幻感的场景(Cheese Daddy&apos;s Landscapes mix)
富有高级感的平面设计(Graphic design_2.0)


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Thu, 09 May 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/05/stable_diffusion_webui-%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/05/stable_diffusion_webui-%E6%A8%A1%E5%9E%8B%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</guid>
        
      <category>技术_AI绘画</category>
        
        
    </item>
    
    <item>
      <title>stable_diffusion_webui 插件扩展笔记</title>
      <description>&lt;h3 id=&quot;悬上该有的天真&quot;&gt;悬上该有的天真。&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;扩展：

推荐的八款插件：

【初学者入门类】

1.中文本地化安装包。

2.图库浏览器。

3.提示词自动补全。

4.Tagger提示词反推。

【进阶提升类】

5.UItimate Upscale 图片绘制修复、放大、优化算法。

6.局部潜空间放大。

7.提示词分割控制。

8.无限放大视频制作。

webui上安装或git安装：

例如：双语：https://github.com/journey-ad/sd-webui-bilingual-localization

git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients extensions/aesthetic-gradients

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Wed, 08 May 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/05/stable_diffusion_webui-%E6%8F%92%E4%BB%B6%E6%89%A9%E5%B1%95%E7%AC%94%E8%AE%B0/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/05/stable_diffusion_webui-%E6%8F%92%E4%BB%B6%E6%89%A9%E5%B1%95%E7%AC%94%E8%AE%B0/</guid>
        
      <category>技术_AI绘画</category>
        
        
    </item>
    
    <item>
      <title>stable_diffusion_webui 安装运行笔记</title>
      <description>&lt;h3 id=&quot;我主张克制不了就放任&quot;&gt;我主张克制不了就放任。&lt;/h3&gt;

&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/AUTOMATIC1111/stable-diffusion-webui&quot;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-安装-cuda-toolkit&quot;&gt;1. 安装 CUDA Toolkit&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;项目指定 CUDA Toolkit 版本 11.8 （必须，高低都不行，我12.0,12.1,12.4都试过！）注：NVIDIA 驱动版本与此无关，无需更改。    

cmd打开命令行，查看当前显卡支持版本
    nvidia-smi
如下图，我的显卡是4070Ti，当前最高可支持12.4版本，我们需要安装11.8版本，所以&amp;gt;=11.8即可。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/1.png&quot; alt=&quot;&quot; width=&quot;60%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;查看当前系统正在使用的版本
    nvcc -V
如下图，我当前使用版本既是11.8，如果不是，请下载安装。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/4.png&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;CUDA Toolkit 下载地址：&lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot;&gt;https://developer.nvidia.com/cuda-toolkit-archive&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/2.png&quot; alt=&quot;&quot; width=&quot;60%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;点击版本11.8，选择符合当前系统的版本下载安装即可，比如我的是 windows 11。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/3.png&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;安装完成使用 nvcc -V 验证是否成功。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-安装-cudnn&quot;&gt;2. 安装 cuDNN&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;安装与 CUDA Toolkit 11.8 匹配的cuDNN版本，这里选择作者推荐版本 
Download cuDNN v8.9.5 (October 27th, 2023), for CUDA 11.x  
下载压缩包
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;cuDNN 下载地址：&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-archive&quot;&gt;https://developer.nvidia.com/rdp/cudnn-archive&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/5.png&quot; alt=&quot;&quot; width=&quot;60%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;解压后的文件&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/6.png&quot; alt=&quot;&quot; width=&quot;60%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;复制这3个文件夹到 CUDA Toolkit 的安装目录中即可，
比如我的安装目录是 D:\NVIDIA\v11.8，可以发现CUDA Toolkit安装目录中也有这三个文件夹，复制过来，相同文件夹中的文件会合并。这种安装方式称之为 插入式安装。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/7.png&quot; alt=&quot;&quot; width=&quot;60%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-安装-python环境&quot;&gt;3. 安装 python环境&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;项目指定 python 版本 3.10.6 （必须，高低都不行）  
cmd打开命令行，查看系统python版本 
    python -V
如果不是：
方法1，卸载系统python，重新安装3.10.6版本的python。
方法2，使用conda创建一个python=3.10.6版本的虚拟环境。

我们使用第二种：
创建名称为 env_sd_webui 的虚拟环境 ： conda create -n env_sd_webui python=3.10.6

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-下载代码&quot;&gt;4. 下载代码&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;下载代码，执行
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;5-修改配置文件&quot;&gt;5. 修改配置文件&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;如果python使用方法2安装的，如下操作，如果使用方法1，跳过此步骤
修改webui-user.bat 文件：
打开代码，找到webui-user.bat 文件

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/8.png&quot; alt=&quot;&quot; width=&quot;60%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
修改 webui-user.bat文件，添加我们创建的python环境地址：

set PYTHON=
set GIT=
set VENV_DIR=
set COMMANDLINE_ARGS=
-》
set PYTHON=D:\Anaconda\envs\env_sd_webui\python.exe
set GIT=
set VENV_DIR=
set COMMANDLINE_ARGS=

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;6-执行程序&quot;&gt;6. 执行程序&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;双击  webui-user.bat 执行脚本。脚本执行时会在项目根路径下创建venv的环境，安装大量依赖包和sd的基础模型，下载量很大，需要十几分钟+。执行成功浏览器窗口自动打开，如下图：
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240507/9.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Tue, 07 May 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/05/stable_diffusion_webui-%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C%E7%AC%94%E8%AE%B0/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/05/stable_diffusion_webui-%E5%AE%89%E8%A3%85%E8%BF%90%E8%A1%8C%E7%AC%94%E8%AE%B0/</guid>
        
      <category>技术_AI绘画</category>
        
        
    </item>
    
    <item>
      <title>UVR5提取干声</title>
      <description>&lt;h3 id=&quot;时间给出了答案&quot;&gt;时间给出了答案。&lt;/h3&gt;

&lt;p&gt;一首音乐大多数由 人声（干声）+ 混响 + 和声 + 伴奏  组成，将各个部分分离。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;原曲分离为 伴奏 + 带和声混响的人声 &lt;br /&gt;
选项卡选择 method = MDX-Net，segment size = 512，overlap = 30，model = MDX23C-InstVoc HQ&lt;br /&gt;
这一步最耗时，根据计算机性能不同，可能需要十几分钟或几十分钟。&lt;br /&gt;
 &lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/9.jpg&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt;
 &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;带和声混响的人声分离为 和声 + 混响人声&lt;br /&gt;
选项卡选择 method = VR Architecture，window size = 320，aggression setting = 8，&lt;br /&gt;
model = 5_HP-Karaoke-UVR(激进)\model = 6_HP-Karaoke-UVR(平滑)&lt;br /&gt;
（注：图中选择了Vocals Only,表示舍弃了和声，请取消勾选，将会得到和声文件）&lt;br /&gt;
 &lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/a.jpg&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt;
 &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;混响人声分离混响 &lt;br /&gt;
选项卡选择 method = VR Architecture，window size = 320，aggression setting = 8， 
model = UVR-De-Echo-Normal，最后得到文件即为人声文件。&lt;br /&gt;
（注：图中选择了No Echo Only，表示只需要没有混响的部分。取消勾选，将会得到混响文件）&lt;br /&gt;
 &lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/b.jpg&quot; alt=&quot;&quot; width=&quot;80%&quot; /&gt;&lt;br /&gt;
 &lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;如果最后的人声文件还存在明显的噪声，再选择 model = UVR-DeNoice去噪。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;经过处理后最终得到 伴奏 + （和声，未勾选Vocals Only ） + （混响，未勾选No Echo Only） + 干声。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/02/so-vits-svc-UVR5%E6%8F%90%E5%8F%96%E5%B9%B2%E5%A3%B0/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/02/so-vits-svc-UVR5%E6%8F%90%E5%8F%96%E5%B9%B2%E5%A3%B0/</guid>
        
      <category>技术_AI语音</category>
        
        
    </item>
    
    <item>
      <title>so-vits-svc 执行笔记</title>
      <description>&lt;h3 id=&quot;风能否向月而行&quot;&gt;风能否向月而行。&lt;/h3&gt;

&lt;h2 id=&quot;效果展示&quot;&gt;效果展示&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1Ym411m7Un/?vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【AI-胡桃】 翻唱 - 丫头&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.bilibili.com/video/BV1aC41187Vr/?spm_id_from=333.999.0.0&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【AI-芙宁娜】 翻唱 - 轻涟（中文）&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.bilibili.com/video/BV1tA4m1P7YF/?spm_id_from=333.788.recommend_more_video.2&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【AI-芙宁娜】 翻唱 - ありがとう（泪的告白）&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.bilibili.com/video/BV1GZ421J7Dd/?spm_id_from=333.788&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;【AI-芙宁娜】 翻唱 - 堕&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;相关教程和参考资料&quot;&gt;相关教程和参考资料&lt;/h2&gt;

&lt;p&gt;so-vits-svc 项目地址： &lt;a href=&quot;https://github.com/svc-develop-team/so-vits-svc.git&quot;&gt;https://github.com/svc-develop-team/so-vits-svc.git&lt;/a&gt; &lt;br /&gt;
官方 README 文档：&lt;a href=&quot;https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/README_zh_CN.md&quot;&gt;https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/README_zh_CN.md&lt;/a&gt;&lt;br /&gt;
B 站 up Sucial的非常详细的教学视频： &lt;a href=&quot;https://www.bilibili.com/video/BV1Hr4y197Cy?p=1&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;https://www.bilibili.com/video/BV1Hr4y197Cy?p=1&amp;amp;vd_source=98a6ce1d2586467c3641a8b5aac049ed&lt;/a&gt;&lt;br /&gt;
教学视频的笔记： &lt;a href=&quot;https://github.com/SUC-DriverOld/so-vits-svc-Chinese-Detaild-Documents?tab=readme-ov-file&quot;&gt;https://github.com/SUC-DriverOld/so-vits-svc-Chinese-Detaild-Documents?tab=readme-ov-file&lt;/a&gt;&lt;br /&gt;
B 站 up Sucial的UVR5人声分离教程: &lt;a href=&quot;https://www.bilibili.com/video/BV1F4421c7qU/?vd_source=98a6ce1d2586467c3641a8b5aac049ed&quot;&gt;https://www.bilibili.com/video/BV1F4421c7qU/?vd_source=98a6ce1d2586467c3641a8b5aac049ed&lt;/a&gt;&lt;br /&gt;
一些报错的解决办法（来自 B 站 up：羽毛布団）: &lt;a href=&quot;https://www.bilibili.com/read/cv22206231/&quot;&gt;https://www.bilibili.com/read/cv22206231/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;关于环境配置&quot;&gt;关于环境配置&lt;/h2&gt;

&lt;p&gt;需要 显存 6G+，并设置虚拟内存 20G+；&lt;/p&gt;

&lt;p&gt;需要 NVIDIA-CUDA，版本11.7,11.8或版本12，请安装。&lt;br /&gt;
查看cuda 版本&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvcc -V   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240221/6.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;需要 安装 visual studio 2022，组件里装“使用 c++的桌面开发”。否则依赖包 fairseq 会安装失败。&lt;/p&gt;

&lt;p&gt;需要环境管理系统 conda，请安装。&lt;/p&gt;

&lt;p&gt;源码下载4.1-Stable稳定版 &lt;a href=&quot;https://github.com/svc-develop-team/so-vits-svc.git&quot;&gt;https://github.com/svc-develop-team/so-vits-svc.git&lt;/a&gt;&lt;br /&gt;
vsCode 打开项目，在控制台使用 conda 创建虚拟环境，并启用&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//创建名称为 env_so_vits_svc 的虚拟环境并安装python 3.9版本。  
conda create -n env_so_vits_svc python=3.9  
//查看是否创建成功
conda env list
//启用环境
conda activate env_so_vits_svc

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;修改 requirements_win.txt 文件，
将 gradio&amp;gt;=3.7.0 改为 gradio==3.41.2，
将 scipy==1.7.3 改为 scipy==1.13.0，（解决与numpy的版本不匹配警告问题）
并添加三个依赖包 fastapi==0.84.0  pydantic==1.10.12  tensorflow&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gradio==3.41.2  
scipy==1.13.0
fastapi==0.84.0  
pydantic==1.10.12 
//用于辅助判断训练结果
tensorflow
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后安装依赖包 (windows)&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    pip install -r requirements_win.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;安装Pytorch， 版本要与CUDA对应，Pytorch 官网&lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240221/7.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;验证是否安装成功&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    python
    # 回车运行
    import torch
    # 回车运行
    print(torch.__version__)
    # 回车运行
    print(torch.cuda.is_available())
    # 回车运行
    //最后一行出现True则成功，出现False则失败，需要重新安装.  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240221/8.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;音频数据集准备&quot;&gt;音频数据集准备&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;音频格式为wav，通过 Adobe Audition 的持续时间排序功能，删除时长 比较短的和比较长的，（回到原文件夹再次删除所有音频，由于Adobe Audition程序占用着没有删除的音频，从而无法删除占用音频，但是能够删除在Adobe Audition 中删除的音频，以此达到我们的过滤删除目的）每条时长在5s～15s之间最佳，过长容易爆内存，保留150条数据以上，然后拖拽所有文件到下方，选中所有，匹配响度。关闭Adobe Audition，保存所有。 （注：可以通过windows资源管理器直接删除不符合要求的音频，选择 排序-》分组依据-》大小，即可对音频重新排序，比AU简单方便。）
&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240221/1.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;回到原文件夹，删除因为匹配响度而产生的所有.pkf文件.&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240221/11.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;将数据集放入项目中的 dataset_raw 目录，数据集以人名命名即可，不可为中文,原则上可同时训练多个人的声音模型，我同时训练了3个，出现了节奏不对的问题，不知道是否是因为同时训练产生的。建议单独训练，且训练1个和多个的时间是累加关系，所以同时训练多个并不会节省时间。
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; dataset_raw
 ├───speaker0
 │   ├───xxx1-xxx1.wav
 │   ├───...
 │   └───Lxx-0xx8.wav
 └───speaker1
     ├───xx2-0xxx2.wav
     ├───...
     └───xxx7-xxx007.wav
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;在dataset_raw下创建config.json文件。
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &quot;n_speakers&quot;: 1

 &quot;spk&quot;:{
     &quot;文件夹名字（不可为中文）&quot;: 0
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/posts/20240221/2.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;  &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;预先下载的模型文件&quot;&gt;预先下载的模型文件&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;使用 contentvec 作为声音编码器（默认且推荐）
下载 contentvec ：checkpoint_best_legacy_500.pt &lt;a href=&quot;https://ibm.ent.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr&quot;&gt;https://ibm.ent.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr&lt;/a&gt; 放在pretrain目录下.&lt;br /&gt;
 或者下载下面的 ContentVec，大小只有 199MB，但效果相同:&lt;br /&gt;
 contentvec ：hubert_base.pt &lt;a href=&quot;https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt&quot;&gt;https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt&lt;/a&gt; , 将文件名改为checkpoint_best_legacy_500.pt后，放在pretrain目录下.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;预训练底模文件： G_0.pth, D_0.pth 放在logs/44k目录下.&lt;br /&gt;
扩散模型预训练底模文件： model_0.pt 放在logs/44k/diffusion目录下.&lt;br /&gt;
&lt;a href=&quot;https://huggingface.co/Sucial/so-vits-svc4.1-pretrain_model&quot;&gt;https://huggingface.co/Sucial/so-vits-svc4.1-pretrain_model&lt;/a&gt;包含G_0.pth，D_0.pth，model_0.pt.s&lt;br /&gt;
预训练的 NSF-HIFIGAN 声码器 ：nsf_hifigan_20221211.zip &lt;a href=&quot;https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip&quot;&gt;https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip&lt;/a&gt;&lt;br /&gt;
解压后，将四个文件放在pretrain/nsf_hifigan目录下&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;程序执行&quot;&gt;程序执行&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;重采样至 44100Hz 单声道,添加–skip_loudnorm 跳过响度匹配步骤.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python resample.py --skip_loudnorm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;执行成功后，.\dataset\44k 目录下将生成音频文件，如下图&lt;br /&gt;
 &lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/3.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;自动划分训练集、验证集，以及自动生成配置文件，增加–vol_aug 参数使用响度嵌入。
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python preprocess_flist_config.py --speech_encoder vec768l12 --vol_aug
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;运行完毕后 .\configs\ 目录下将生成 config.json 与 diffusion.yaml 文件&lt;br /&gt;
 此时可以在生成的 config.json 与 diffusion.yaml 修改部分参数&lt;br /&gt;
     keep_ckpts：训练时保留最后几个模型，0为保留所有，默认只保留最后3个。&lt;br /&gt;
     batch_size：单次训练加载到 GPU 的数据量，调整到低于显存容量的大小即可,默认是6，6g显存的需调为3比较好。&lt;/p&gt;

    &lt;p&gt;如下图&lt;br /&gt;
 &lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/4.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;生成 hubert 与 f0。尚若需要浅扩散功能（可选），需要增加–use_diff 参数.执行需要一定时间。&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python preprocess_hubert_f0.py --f0_predictor dio --use_diff
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;执行成功后，每个音频文件都生成了5个相关的文件，如下图&lt;br /&gt;
 &lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/5.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;主模型训练，需要epoch 5000+，追求效果建议epoch &amp;gt; 9000,训练时间与训练集大小和显卡性能相关，3小时数据集 + 4070Ti 12G显卡+ epoch=10000，大约需要120小时。训练过程中 可ctrl+c中断，继续训练再次执行训练命令即可。&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python train.py -c configs/config.json -m 44k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;扩散模型训练（可选），测试效果：我没听出来有啥变化。&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python train_diff.py -c configs/diffusion.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;模型训练结束后，模型文件保存在logs/44k目录下，扩散模型在logs/44k/diffusion下。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;模型压缩/打包分享&lt;/p&gt;

    &lt;p&gt;生成的模型含有继续训练所需的信息。如果确认不再训练，可以移除模型中此部分信息，得到约 1/3 大小的最终模型。&lt;br /&gt;
 生成模型大小在600M~800M，压缩后200M左右。&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python compress_model.py -c=&quot;configs/config.json&quot; -i=&quot;logs/44k/G_30400.pth&quot; -o=&quot;logs/44k/release.pth&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;训练结果的判断
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// 启动 tensorboard ，命令行输入
python -m tensorboard.main --logdir=logs\44k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;命令行返回一个本地地址，浏览器打开地址，需要等待训练200步之后才会出现表格。此时命令行窗口不能关闭。 
如何判断训练的差不多了？找到 loss 标签下的 loss/g/lf0，当图像趋于稳定的直线时，则说明训练的差不多了，此时去推理听一听结果在做判断。下图是训练十万步的结果。 
 &lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/c.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由于命令行开启 tensorboard 会占用命令行窗口，写一个 .bat 执行文件，专门用于打开tensorboard，命名为 启动tensorboard.bat 即可，放在项目根目录下，双击点开即可&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    chcp 65001
    @echo off

    echo 正在启动Tensorboard...
    echo 如果看到输出了一条网址（大概率是localhost:6006）就可以访问该网址进入Tensorboard了

    //D:\Anaconda\envs\env_so_vits_svc 替换这个地址为自己的conda环境地址
    D:\Anaconda\envs\env_so_vits_svc\python.exe -m tensorboard.main --logdir=logs\44k

    pause
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
    &lt;img src=&quot;/images/posts/20240221/e.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;目标音频处理-uvr5工具&quot;&gt;目标音频处理 uvr5工具&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://yuqianglianshou.com/2024/02/so-vits-svc-UVR5提取干声/&quot;&gt;UVR5提取干声&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;推理&quot;&gt;推理&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;使用以下命令打开 webui 界面,推理期间，此命令行窗口不能关闭。
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python webUI.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;由于命令行开启 webui 界面会占用命令行窗口，写一个 .bat 执行文件，专门用于打开推理界面，命名为 启动 webui.bat 即可，放在项目根目录下，双击点开即可&lt;/p&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     chcp 65001
     @echo off

     echo 初始化并启动WebUI……初次启动可能会花上较长时间
     echo WebUI运行过程中请勿关闭此窗口！

     //D:\Anaconda\envs\env_so_vits_svc 替换这个地址为自己的conda环境地址
     D:\Anaconda\envs\env_so_vits_svc\python.exe webUI.py

     pause
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;&lt;br /&gt;
 &lt;img src=&quot;/images/posts/20240221/d.jpg&quot; alt=&quot;&quot; width=&quot;90%&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;模型选择 .\logs\44k\G_步数.pth 文件，配置文件选择 .\configs\config.json&lt;br /&gt;
下面的音频选择 选择处理好的干声。&lt;br /&gt;
其他参数不需要动。&lt;/li&gt;
  &lt;li&gt;点击音频转换，生成推理后的音频。&lt;/li&gt;
  &lt;li&gt;通过其他软件比如 剪映 合成 推理后的音频 + 伴奏，完成。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;关于文字转音频，点击文字转音频，输入文本，勾选上方F0预测，点击转换即可。&lt;/p&gt;

&lt;h2 id=&quot;关于-conda&quot;&gt;关于 conda&lt;/h2&gt;

&lt;p&gt;Conda是一个开源的软件包管理系统和环境管理系统，用于安装和管理软件包及其依赖项。它最初是为Python语言设计的，但也可用于其他语言和工具。Conda允许用户轻松地创建、分享、管理和部署环境和软件包，使开发人员能够更有效地管理项目的依赖关系。&lt;/p&gt;

&lt;p&gt;Conda的主要特点包括：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;软件包管理：Conda可以安装、更新、卸载和管理软件包及其依赖项。&lt;/li&gt;
  &lt;li&gt;环境管理：Conda允许用户创建多个隔离的环境，每个环境都可以拥有不同版本的软件包，从而使得不同项目之间的依赖关系可以互相隔离，避免冲突。&lt;/li&gt;
  &lt;li&gt;跨平台性：Conda支持在不同的操作系统上运行，包括Windows、macOS和Linux。&lt;/li&gt;
  &lt;li&gt;开源和社区支持：Conda是开源的，有一个活跃的社区支持，用户可以分享和贡献软件包和环境。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总的来说，Conda是一个强大而灵活的工具，可以帮助开发人员更轻松地管理项目的依赖关系，提高开发效率。&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//conda 的版本查看
conda -V

//存在的conda环境查看，正在启用的环境会有*标。
conda info --envs
或者
conda envs list

//创建 名为 pytorch_gpu, python版本为3.9 的虚拟环境
conda create -n pytorch_gpu python=3.9 

//启用名为 pytorch_gpu 的环境，
conda activate pytorch_gpu 

//取消正在启用环境
conda deactivate

//删除创建的环境
conda env remove --name &amp;lt;环境名称&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;关于-问题建议&quot;&gt;关于 问题、建议&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;音频数据集理论上越多越好，越多也就意味着训练模型时的时间成本越高。&lt;/li&gt;
  &lt;li&gt;音频数据集质量（清晰度、有无杂音）很重要，直接影响结果。&lt;/li&gt;
  &lt;li&gt;音频数据集的音调范围，数据集中有高音、低音的样本，在推理后才会有相应的结果，比如数据集中没有高音的样本，推理了一首高音的歌曲，会出现哑音、唱不上去的现象。&lt;/li&gt;
  &lt;li&gt;音频数据集的数据后缀需要小写的 .wav，大写的（.WAV）项目不支持，要注意。&lt;/li&gt;
  &lt;li&gt;尽量找和模型音色相近的音乐干声。相近的和差距很大的音色最后合成的效果差距很大。&lt;/li&gt;
  &lt;li&gt;epoch 5000+以上，测试想要一个不错的结果训练时常需要100个小时以上。epoch次数在log里查看。&lt;/li&gt;
  &lt;li&gt;我尝试了一次训练了3个模型，结果推理后的歌曲有两个模型有节拍不同步的问题，不知道具体原因是什么，建议一次只训练一个模型。&lt;/li&gt;
  &lt;li&gt;训练过程中关闭不用的程序等减少内存占用，其他程序的cpu、gpu的使用，会导致训练epoch的时间增长。推理的使用和tensorboard的使用也会这样，不用时及时关闭，保证电脑资源尽量多的给到训练程序。&lt;/li&gt;
  &lt;li&gt;需要训练的声音要有足够的自己的声音特色。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/02/so-vits-svc-%E6%89%A7%E8%A1%8C%E7%AC%94%E8%AE%B0/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/02/so-vits-svc-%E6%89%A7%E8%A1%8C%E7%AC%94%E8%AE%B0/</guid>
        
      <category>技术_AI语音</category>
        
        
    </item>
    
    <item>
      <title>AI语音_知识点整理</title>
      <description>&lt;h3 id=&quot;乐观是极度的绝境之中唯一的武器也是最后的救命稻草&quot;&gt;乐观是极度的绝境之中唯一的武器，也是最后的救命稻草。&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;声音的基本属性：&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;音色（Timbre）：音色是声音的质地或音质，是使不同乐器或人的声音听起来不同的特征。即使两个声音具有相同的音调（频率）和响度（振幅），它们的音色也可能不同。
    音色是由声波的谐波成分和频谱分布所决定的，不同乐器或声源产生的声波谐波成分和频谱分布不同，因此具有不同的音色。

音调（Pitch）：音调是声音的频率属性，表示声波振动的快慢程度。频率高的声音听起来比频率低的声音更尖锐。音调与声音的音高相关，高频率对应高音调，低频率对应低音调。
    在音乐中，音调通常与音符的高低对应，用来描述音符的音高。

响度（Loudness）：响度是声音的振幅或强度属性，表示声波振动的幅度大小，也称为音量。振幅大的声音听起来更响亮，振幅小的声音听起来更柔和。
    响度通常与声音的音量相关，较高的振幅对应更高的音量。在音乐中，响度常用来描述音符的强弱或音乐的整体音量。

    响度通常以分贝（dB）为单位进行描述，分贝是一种对声音强度的对数测量单位，它基于声压级（声压的对数比）来表示声音的相对强度。

    通常情况下，人类能够感知的响度范围非常广泛，从非常微弱的声音，比如安静的耳语（约20分贝），到非常响亮的声音，比如飞机发动机的轰鸣（约120分贝）。

    以下是一些常见的声音响度范围及其描述：

    0-20 分贝：极其安静，类似于安静的图书馆或夜晚的室外。
    20-40 分贝：安静，类似于低声细语或轻微的风声。
    40-60 分贝：一般的谈话声或办公室环境。
    60-80 分贝：相对较大的声音，如电视机声音、交通噪音或餐馆内的谈话声。
    80-100 分贝：较大的声音，如汽车喇叭声、音乐会的声音或运动场上的欢呼声。
    100-120 分贝：非常响亮的声音，如摩托车引擎声或工厂机器声。
    120 分贝以上：极端响亮的声音，如飞机发动机声、火箭发射声。
    响度的测量通常是以标准声压级（20微帕）作为基准。因此，一个声音的响度可以被描述为相对于标准声压级的对数比，并以分贝为单位。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;TTS 和 VITS 的区别&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;文本到语音（TTS）和Voice Inverse Text-to-Speech（VITS）是两种不同的语音合成方法，它们在原理和应用方面有一些显著的区别：

1. **输入数据**：
   - TTS：TTS系统接收文本作为输入，然后生成对应的语音输出。它将文本转换为声音，并通常包括文本预处理、语音合成和音频后处理等步骤。
   - VITS：VITS直接从声音信号到声音信号进行训练，跳过了中间的文本表示。它不需要文本输入，而是直接处理声音信号。

2. **训练方法**：
   - TTS：TTS系统通常使用文本语料库和对应的语音语料库进行训练。它需要大量的文本-语音配对数据来学习文本到语音的映射关系。
   - VITS：VITS训练的输入和输出都是声音信号，它可以直接从声音信号对之间的关系中学习，而无需使用文本-语音配对数据。

3. **应用场景**：
   - TTS：TTS系统通常用于将文本转换为语音，例如语音助手、语音播报、自动电话系统等应用。
   - VITS：VITS更适用于声音信号处理的任务，例如声音合成、声音增强、语音转换等应用。

4. **训练复杂度**：
   - TTS：由于TTS系统需要大量的文本-语音配对数据进行训练，因此其训练复杂度较高。
   - VITS：VITS不需要文本-语音配对数据，可以直接从声音信号中学习，因此其训练复杂度较低。

总的来说，TTS和VITS是两种不同的语音合成方法，它们在输入数据、训练方法、应用场景和训练复杂度等方面有所不同，可以根据具体的需求选择合适的方法。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;F0预测器中 crepe,pm,dio,harvest,rmvpe  的区别&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;F0预测器用于从音频信号中提取基频（F0）或基频轮廓，这对于语音合成、变声和其他音频处理应用至关重要。以下是您提到的几个F0预测器的简介及其主要区别：

### 1. CREPE
- **全称**：Convolutional Recurrent Neural Network for Pitch Estimation
- **方法**：基于卷积神经网络（CNN）和递归神经网络（RNN）。
- **特点**：CREPE直接对音频波形进行操作，能够提供高精度的基频估计。它不依赖于传统的时域或频域处理方法，而是使用深度学习来提取音频信号的特征。
- **优点**：在嘈杂环境下表现良好，适合多种音频信号的基频提取。
- **缺点**：计算资源需求较高，需要大量训练数据。

### 2. PM (Parabolic Interpolation)
- **全称**：Parabolic Interpolation Method
- **方法**：利用傅里叶变换和抛物线插值来估计基频。
- **特点**：这是一个基于频域的方法，通过在频谱上找到峰值并进行抛物线插值来精确定位基频。
- **优点**：计算简单，适用于实时应用。
- **缺点**：在处理复杂信号或低信噪比信号时，精度可能较低。

### 3. DIO
- **全称**：Distributed Inline Operation
- **方法**：一种基于自相关函数和周期图的基频估计算法。
- **特点**：DIO是一种快速而精确的基频提取方法，广泛应用于语音合成领域。
- **优点**：计算效率高，能够在短时间内处理大量数据。
- **缺点**：在处理含有高噪声或谐波丰富的信号时，可能会有一定的误差。

### 4. Harvest
- **方法**：一种基于时间域和频域的混合方法，用于精确的基频估计。
- **特点**：Harvest结合了自相关方法和谐波增强技术，以提高基频估计的准确性。
- **优点**：能够在较低信噪比的环境下提供可靠的基频估计。
- **缺点**：计算复杂度较高，相对于简单的方法计算速度较慢。

### 5. RMVPE
- **全称**：Robust Multi-Voice Pitch Estimation
- **方法**：基于机器学习或深度学习的方法，专门设计用于处理多声部或复杂声源的基频提取。
- **特点**：RMVPE能够在多声部音乐或合唱中准确分离和提取每个声部的基频。
- **优点**：在复杂音频环境中表现出色，适用于多声源信号。
- **缺点**：模型复杂，训练和推理过程需要大量计算资源。

### 总结
- **CREPE**：高精度、深度学习方法、适合复杂和嘈杂环境。
- **PM**：基于抛物线插值、计算简单、适合实时应用。
- **DIO**：高效快速、基于自相关函数和周期图。
- **Harvest**：混合时间域和频域方法、精度高、计算复杂度高。
- **RMVPE**：适用于多声部音频、机器学习方法、适合复杂声源环境。

选择适合的F0预测器取决于具体的应用需求、计算资源以及所处理音频信号的复杂程度。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Github 项目 vits&lt;/strong&gt;   &lt;a href=&quot;https://github.com/jaywalnut310/vits&quot;&gt;GitHub地址：https://github.com/jaywalnut310/vits&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
VITS 是 https://github.com/RVC-Boss/GPT-SoVITS 的基础框架。 

VITS是一种基于自注意力机制（self-attention）的声音合成方法。以下是该项目的基本原理：

1. **自注意力机制（Self-Attention）**：自注意力机制是一种用于处理序列数据的机制，它可以根据输入序列中各个元素之间的关系来动态地调整每个元素的权重。
在语音合成中，自注意力机制可以帮助模型捕捉声音信号中不同部分之间的相关性和依赖关系。

2. **声音合成**：VITS是一种端到端的声音合成方法，它直接从声音信号到声音信号进行训练和生成，跳过了中间的文本表示。这种方法可以更好地保留声音的特征和表达，
从而产生更加自然和高质量的声音合成结果。

3. **模型架构**：VITS模型使用了自注意力机制来建模声音信号之间的长距离依赖关系，并通过学习声音信号的高级表示来实现声音合成。该模型可能包括编码器和解码器部分，
编码器负责将输入声音信号编码为高级表示，而解码器则负责根据这些表示生成输出声音信号。

4. **训练方法**：VITS模型可能使用了大量的声音信号数据进行训练，通过最小化声音合成时的预测误差来优化模型参数。训练过程可能会使用自注意力机制和梯度下降等技术来实现。

通过以上原理，VITS能够实现高质量的端到端声音合成，无需依赖于中间的文本表示，适用于多种语音合成和声音处理任务。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Github 项目 ar-vits&lt;/strong&gt;  &lt;a href=&quot;https://github.com/innnky/ar-vits&quot;&gt;GitHub地址：https://github.com/innnky/ar-vits&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
AutoRegressive-VITS（自回归-VITS）

AR-VITS 是 https://github.com/RVC-Boss/GPT-SoVITS 的基础框架。 

AR-VITS是一种用于声音合成的模型。以下是该项目的基本原理：

1. **AutoRegressive (AR)**：
AutoRegressive是指使用过去时间步的输出作为输入来预测下一个时间步的模型。在声音合成中，这意味着模型会考虑之前的声音样本来预测下一个时间步的声音。

1. **VITS**：
VITS是Voice Inverse Text-to-Speech的缩写，是一种端到端的声音合成方法。与传统的文本到语音（TTS）系统不同，VITS直接从声音信号到声音信号进行训练，
跳过了中间的文本表示。这种方法可以更好地保留声音的特征和表达。

结合两者，AR-VITS可能是一个结合了自回归模型和VITS方法的声音合成模型。它可能使用自回归模型来建模声音信号之间的时间依赖关系，
并结合VITS的思想来直接从声音信号到声音信号进行训练和生成，从而实现更加自然和高质量的声音合成。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Github 项目 hifi-gan&lt;/strong&gt;  &lt;a href=&quot;https://github.com/jik876/hifi-gan&quot;&gt;GitHub地址：https://github.com/jik876/hifi-gan&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://github.com/RVC-Boss/GPT-SoVITS 的声码器。 

HiFi-GAN 是一个基于生成对抗网络（GAN）的高保真度语音合成模型。以下是该项目的基本讲解：

1. **目标**：
HiFi-GAN旨在实现高保真度的语音合成，即生成具有高质量、高逼真度的语音信号，使其听起来与真实人类语音相似。

1. **生成对抗网络（GAN）**：
HiFi-GAN使用生成对抗网络（GAN）作为基础架构。GAN由两个主要部分组成：生成器（Generator）和判别器（Discriminator）。生成器负责生成高保真度的语音信号，
而判别器则负责区分生成的语音信号和真实的语音信号。通过对抗训练的方式，生成器逐渐学习生成高质量的语音信号，而判别器则逐渐学习提高其鉴别能力。
生成器是一个神经网络模型，它接收随机噪声作为输入，并尝试生成高保真度的语音信号。通常，生成器会使用深度卷积神经网络（CNN）或长短期记忆网络（LSTM）等结构来实现。
判别器也是一个神经网络模型，它接收两种输入：真实语音信号和生成器生成的合成语音信号。判别器的目标是区分这两种输入，并尽可能准确地判断生成的语音信号是否与真实语音信号相似。

1. **高保真度语音合成**：
HiFi-GAN采用了一系列技术和创新，包括使用长短期记忆（LSTM）或卷积神经网络（CNN）作为生成器的结构，引入了自适应特征正则化（Adaptive Feature Regularization）
等方法来提高合成语音的质量和逼真度。这些技术和方法帮助HiFi-GAN生成接近真实人类语音的高保真度语音信号。

1. **训练和优化**：
HiFi-GAN通过对真实语音数据进行训练，使用GAN的对抗训练方法来不断优化生成器和判别器的参数。训练过程中可能会使用一系列技巧和策略来提高模型的性能和收敛速度。

1. **应用场景**：
HiFi-GAN可以应用于多种语音合成和语音处理任务，包括语音合成、语音增强、语音转换等。它可以用于语音助手、语音合成系统、语音转文字等领域。

通过以上讲解，你可以了解到HiFi-GAN是一个基于GAN的高保真度语音合成模型，它通过对抗训练的方式来生成高质量、高逼真度的语音信号，适用于多种语音处理和合成任务。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Github 项目 SoundStorm&lt;/strong&gt;  &lt;a href=&quot;https://github.com/yangdongchao/SoundStorm&quot;&gt;GitHub地址：https://github.com/yangdongchao/SoundStorm&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
https://github.com/RVC-Boss/GPT-SoVITS 的类GPT（AR）模型 https://github.com/yangdongchao/SoundStorm/tree/master/soundstorm/s1/AR。 

SoundStorm是一个基于WaveNet的音频合成系统，它可以生成高质量的语音音频。以下是该项目的基本讲解：

1. **WaveNet模型**：
SoundStorm基于WaveNet模型，这是由DeepMind提出的一种深度生成模型，用于生成音频波形。WaveNet模型使用了深度卷积神经网络（CNN）结构，并且具有极高的参数效率，
能够生成高质量、高保真度的音频信号。

1. **自回归模型**：
WaveNet是一种自回归模型，它通过条件概率分布来建模音频波形的生成过程。具体来说，模型通过已知的音频序列来预测下一个样本点的值，然后将其作为输入，依次生成整个音频波形。

1. **音频合成**：
SoundStorm使用WaveNet模型来进行音频合成。给定输入的音频特征（如语音文本、音乐谱、或其他表示），WaveNet模型可以生成对应的高保真度音频波形。

1. **训练和优化**：
训练SoundStorm模型通常需要大量的音频数据，并且可能需要花费大量的时间和计算资源。在训练过程中，模型会通过最小化生成音频与真实音频之间的差异来优化参数。

1. **应用场景**：
SoundStorm可以应用于多种音频合成任务，包括语音合成、音乐合成、声音效果生成等。它可以用于语音合成系统、音乐生成器、声音特效软件等领域。

综上所述，SoundStorm是一个基于WaveNet的音频合成系统，利用WaveNet模型可以生成高质量、高保真度的音频波形，适用于多种音频合成任务。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Github 项目 chinese_speech_pretrain&lt;/strong&gt;  &lt;a href=&quot;https://github.com/TencentGameMate/chinese_speech_pretrain&quot;&gt;GitHub地址：https://github.com/TencentGameMate/chinese_speech_pretrain&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
https://github.com/RVC-Boss/GPT-SoVITS 的Hubert特征。 

chinese_speech_pretrain是腾讯游戏助手团队开发的一个中文语音预训练模型库，旨在提供中文语音相关的预训练模型和工具。以下是该项目的基本讲解：

1. **预训练模型**：
chinese_speech_pretrain提供了一系列中文语音相关的预训练模型，包括语音识别（ASR）、语音合成（TTS）、语音情感识别等模型。
这些预训练模型可以用于各种中文语音相关的任务和应用。

1. **模型结构**：
这些预训练模型可能基于深度学习技术，采用了各种不同的模型结构，如循环神经网络（RNN）、卷积神经网络（CNN）、自注意力机制（Self-Attention）等。
每个模型可能针对特定的任务和应用场景进行了优化和调整。

1. **数据集**：
为了训练这些预训练模型，可能使用了大量的中文语音数据集进行训练。这些数据集可能包括语音识别数据、语音合成数据、语音情感数据等，覆盖了多种不同的语音场景和语音内容。

1. **应用示例**：
chinese_speech_pretrain可能提供了一些示例代码和应用案例，帮助用户理解和使用预训练模型。这些示例代码可以用来进行语音识别、语音合成、语音情感识别等任务的实际操作和测试。

1. **开源社区**：
该项目可能是一个开源项目，允许用户参与到模型的开发和改进中，也可以通过GitHub等平台向开发团队提出问题和反馈建议。

综上所述，chinese_speech_pretrain是一个提供中文语音相关预训练模型和工具的开源项目，用户可以使用这些模型和工具来进行中文语音相关任务的研究和应用。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Github 项目 contentvec&lt;/strong&gt;  &lt;a href=&quot;https://github.com/auspicious3000/contentvec&quot;&gt;GitHub地址：https://github.com/auspicious3000/contentvec&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
https://github.com/RVC-Boss/GPT-SoVITS 的VC用的低音色特征。 

contentvec 是一个用于内容建模的开源库，它提供了一种简单而强大的方法来将文本转换为向量表示。以下是该项目的基本讲解：

1. **内容建模**：
contentvec 的主要目标是将文本内容转换为向量表示，从而实现内容建模。这种向量表示可以捕获文本的语义和语境信息，使得文本可以被更好地理解和处理。

1. **词向量化**：
contentvec 可能使用了词向量化（Word Embedding）技术，将文本中的单词或短语映射到高维向量空间中。通过词向量化，单词之间的语义关系可以在向量空间中得到体现，
从而可以进行更多的语义分析和推断。

1. **模型结构**：
contentvec 可能基于深度学习技术，采用了神经网络模型来学习文本的向量表示。这些模型可能包括循环神经网络（RNN）、卷积神经网络（CNN）、注意力机制（Attention）等结构，
以实现对文本内容的抽象和表示。

1. **训练数据**：
为了训练内容向量模型，contentvec 可能使用了大量的文本数据进行训练。这些数据可能包括各种不同领域和类型的文本，例如新闻文章、社交媒体文本、电子书籍等，
以覆盖尽可能多的语义和语境信息。

1. **应用场景**：
contentvec 的向量表示可以应用于多种文本相关的任务和应用，包括文本分类、情感分析、语义相似度计算、信息检索等。它可以用于搜索引擎、推荐系统、自然语言处理应用等领域。

综上所述，contentvec 是一个用于内容建模的开源库，通过将文本转换为向量表示来捕获文本的语义信息，以应用于各种文本相关的任务和应用中。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Github 项目 fish-speech&lt;/strong&gt;  &lt;a href=&quot;https://github.com/fishaudio/fish-speech&quot;&gt;GitHub地址：https://github.com/fishaudio/fish-speech&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
https://github.com/RVC-Boss/GPT-SoVITS 的概率分布采样 https://github.com/fishaudio/fish-speech/blob/main/tools/llama/generate.py#L41。 

fish-speech是一个基于Python的语音处理库，旨在提供简单易用的接口来进行语音信号的处理、分析和转换。以下是该项目的基本讲解：

1. **功能特性**：fish-speech提供了一系列功能丰富的工具和方法，用于处理语音信号。这些功能包括语音信号的读取和写入、频谱分析、声音特征提取、语音转换、语音识别等。

2. **支持格式**：fish-speech支持多种常见的语音文件格式，包括WAV、MP3、FLAC等，可以方便地读取和处理这些格式的语音文件。

3. **API接口**：fish-speech提供了简单易用的API接口，使得用户可以轻松地调用各种功能来处理语音信号。这些接口设计简洁清晰，方便用户理解和使用。

4. **应用示例**：该项目可能提供了一些示例代码和应用案例，帮助用户理解和使用fish-speech库。这些示例代码可以用来进行语音信号的处理、分析和转换，或者作为学习和参考的资源。

5. **开源社区**：fish-speech是一个开源项目，允许用户参与到开发和改进中，也可以通过GitHub等平台向开发团队提出问题和反馈建议。

综上所述，fish-speech是一个功能丰富的语音处理库，提供了简单易用的接口来进行语音信号的处理、分析和转换，适用于各种语音相关的应用和任务。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;在 Hugging Face 模型中心 chinese-roberta-wwm-ext-large&lt;/strong&gt;  &lt;a href=&quot;https://huggingface.co/hfl/chinese-roberta-wwm-ext-large&quot;&gt;GitHub地址：https://huggingface.co/hfl/chinese-roberta-wwm-ext-large&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
https://github.com/RVC-Boss/GPT-SoVITS 的中文BERT预训练特征。 

https://huggingface.co/hfl/chinese-roberta-wwm-ext-large 是一个在 Hugging Face 模型中心提供的中文 RoBERTa 预训练模型，
它基于 RoBERTa（Robustly optimized BERT approach）模型架构，并在中文语境下进行了预训练。以下是该模型的基本讲解：

1. **RoBERTa 模型**：
RoBERTa 是一个基于 BERT（Bidirectional Encoder Representations from Transformers）的改进模型，它采用了一系列优化策略来提升语言建模的性能和效果。
RoBERTa 在预训练阶段采用了更大的数据集、更长的训练时间和更多的训练步骤，以及一些其他的技术改进，从而在各种 NLP 任务上取得了优秀的性能。

2. **中文 RoBERTa 模型**：
https://huggingface.co/hfl/chinese-roberta-wwm-ext-large 是一个专门针对中文语境进行预训练的 RoBERTa 模型。它使用了来自中文语料库的大量文本数据进行预训练，
以学习中文语言的表示形式，并为各种中文 NLP 任务提供强大的特征表示。

3. **模型架构**：
该模型可能采用了 RoBERTa 的标准模型架构，包括多层的 Transformer 编码器（Transformer Encoder），每个编码器层由多头自注意力机制（Multi-Head Self-Attention）
和前馈神经网络组成。通过多层的堆叠，模型能够捕获输入文本的丰富语义信息。

4. **应用场景**：
中文 RoBERTa 模型可以应用于各种中文自然语言处理任务，包括文本分类、命名实体识别、情感分析、语义理解、文本生成等。它可以作为预训练模型在下游任务上进行微调，
也可以作为特征提取器来获取文本的表示形式。

5. **开源社区支持**：
该模型由 Hugging Face 提供，属于一个开源社区项目，用户可以通过 Hugging Face Transformers 库轻松地加载和使用该模型，
并结合其他工具和库进行各种 NLP 任务的开发和实验。

综上所述，https://huggingface.co/hfl/chinese-roberta-wwm-ext-large 是一个基于 RoBERTa 架构的中文预训练模型，适用于各种中文自然语言处理任务，
并由 Hugging Face 提供支持和维护。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Gradio&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Gradio是一个Python库，用于构建快速的机器学习界面。它允许用户将机器学习模型快速部署为易于使用的交互式界面，无需编写任何前端代码。Gradio提供了简单的API，
使得用户可以轻松地将模型部署为Web应用，以便用户可以通过浏览器与模型进行交互。

Gradio支持各种类型的输入和输出，包括文本、图像、音频、视频等。用户可以使用Gradio来构建各种不同类型的机器学习应用，例如图像分类、目标检测、文本生成、语音识别等。

Gradio提供了一个简单直观的界面，用户可以通过拖放或上传文件来输入数据，然后查看模型的预测结果。它还支持自定义界面布局、样式和交互逻辑，以满足不同应用的需求。

总之，Gradio是一个方便易用的工具，可以帮助用户快速构建和部署机器学习模型的交互式界面，使得模型的应用和使用更加简单和直观。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;PyTorch&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PyTorch是一个用于机器学习和深度学习的开源Python库，它提供了灵活而强大的工具和接口，用于构建和训练各种类型的神经网络模型。以下是关于PyTorch的一些重要特点和功能：

1. **动态计算图**：
PyTorch使用动态计算图来表示神经网络模型，这意味着计算图是在运行时构建的，允许用户使用Python的控制流结构（如循环和条件语句）来定义复杂的计算图。

2. **自动微分**：
PyTorch提供了自动微分功能，可以自动计算神经网络模型中参数的梯度，从而实现反向传播算法进行模型训练。这使得构建和训练神经网络模型变得更加简单和高效。

3. **模块化设计**：
PyTorch的设计是模块化的，提供了丰富的模块和函数，包括各种类型的神经网络层、损失函数、优化器等，可以方便地构建和定制神经网络模型。

4. **GPU加速**：
PyTorch支持在GPU上进行计算，可以利用GPU的并行计算能力加速模型训练和推断过程。用户可以轻松地将模型和数据移动到GPU上进行计算，从而提高计算性能。

5. **丰富的生态系统**：
PyTorch拥有一个庞大而活跃的社区，提供了丰富的资源和工具，包括预训练模型、数据集、教程、论坛等，使得用户可以快速上手并加速开发过程。

总的来说，PyTorch是一个强大而灵活的深度学习框架，具有动态计算图、自动微分、模块化设计、GPU加速等特点，适用于各种机器学习和深度学习任务。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
      <pubDate>Thu, 15 Feb 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/02/AI%E8%AF%AD%E9%9F%B3_%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/02/AI%E8%AF%AD%E9%9F%B3_%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/</guid>
        
      <category>技术_AI语音</category>
        
        
    </item>
    
    <item>
      <title>我的2023年终总结</title>
      <description>&lt;h3 id=&quot;我好像失去了什么又似乎什么也没有拥有过&quot;&gt;我好像失去了什么，又似乎什么也没有拥有过。&lt;/h3&gt;
&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;我好像失去了什么，又似乎什么也没有拥有过。&lt;br /&gt;
我好像失去了什么，又似乎什么也没有拥有过。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt;&lt;/p&gt;
</description>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0800</pubDate>
      <link>http://localhost:4000/2024/01/%E6%88%91%E7%9A%842023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</link>
      <guid isPermaLink="true">http://localhost:4000/2024/01/%E6%88%91%E7%9A%842023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</guid>
        
      <category>人生</category>
        
        
    </item>
    
  </channel>
</rss>
